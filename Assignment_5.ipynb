{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "nGf73cFjioVe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyPDF2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EbqgdkAMhefN",
        "outputId": "1f6d118f-122e-41fe-a09c-f38ad6a22d33"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.10/dist-packages (3.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q/A using Hugging Face Model"
      ],
      "metadata": {
        "id": "xA_4lpbhs_Mn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import PyPDF2\n",
        "from transformers import pipeline, GPT2Tokenizer, GPT2LMHeadModel, pipeline\n",
        "\n",
        "# Function to extract text from a PDF file using PyPDF2\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    with open(pdf_path, 'rb') as file:\n",
        "        pdf_reader = PyPDF2.PdfReader(file)\n",
        "        text = \"\"\n",
        "        for page_num in range(len(pdf_reader.pages)):\n",
        "            page = pdf_reader.pages[page_num]\n",
        "            text += page.extract_text()\n",
        "    return text\n",
        "\n",
        "# Function to answer questions using a question-answering pipeline\n",
        "def answer_questions(pdf_text, question, model_name=\"deepset/bert-large-uncased-whole-word-masking-squad2\"):\n",
        "    qa_pipeline = pipeline(\"question-answering\", model=model_name, tokenizer=model_name)\n",
        "    answer = qa_pipeline(question=question, context=pdf_text)\n",
        "    return answer['answer']\n",
        "\n",
        "# Example usage\n",
        "pdf_path = \"/content/form1-z (1).pdf\"\n",
        "pdf_text = extract_text_from_pdf(pdf_path)\n",
        "\n",
        "# Ask a question based on the PDF content\n",
        "question = \"What is this form1-z about?\"\n",
        "answer = answer_questions(pdf_text, question)\n",
        "\n",
        "# print(\"Extracted Text from PDF:\")\n",
        "# print(pdf_text)\n",
        "\n",
        "print(\"\\nAnswer to the Question:\")\n",
        "print(answer)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Xipsh3CpooB",
        "outputId": "b91a8f8a-0401-44dc-e2e1-b8561a07feaf"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at deepset/bert-large-uncased-whole-word-masking-squad2 were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Answer to the Question:\n",
            "OMB APPROVAL\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai==0.28"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dc7NwINAwQAT",
        "outputId": "c777d939-9acb-4e77-9010-84076955777e"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai==0.28 in /usr/local/lib/python3.10/dist-packages (0.28.0)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (4.66.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (3.8.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2023.7.22)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.3.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Embeddings generation using sentence encoder\n",
        "\n",
        "Universal Sentence Encoder (USE)Â¶\n",
        "The Universal Sentence Encoder encodes text into high-dimensional vectors that are used here for embedding the documents. The model is trained and optimized for greater-than-word length text, such as sentences, phrases, or short paragraphs."
      ],
      "metadata": {
        "id": "ppjoG0IZy-Bx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "VCsfY59_tF6p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import PyPDF2\n",
        "\n",
        "def extract_pdf_text(pdf_path):\n",
        "    try:\n",
        "        # Read the PDF content\n",
        "        with open(pdf_path, 'rb') as file:\n",
        "            pdf_reader = PyPDF2.PdfReader(file)\n",
        "            text = \"\"\n",
        "            for page_num in range(len(pdf_reader.pages)):\n",
        "                page = pdf_reader.pages[page_num]\n",
        "                text += page.extract_text()\n",
        "\n",
        "            return text.strip()\n",
        "\n",
        "    except PyPDF2.utils.PdfReadError as e:\n",
        "        print(f\"Error reading PDF: {e}\")\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred: {e}\")\n",
        "\n",
        "    return None\n",
        "\n",
        "# Example usage with a local file path\n",
        "pdf_path = \"/content/form1-z (1).pdf\"\n",
        "pdf_text = extract_pdf_text(pdf_path)\n",
        "\n",
        "if pdf_text is not None:\n",
        "    print(\"Extracted Text from PDF:\")\n",
        "\n",
        "else:\n",
        "    print(\"Failed to extract text from the PDF.\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Extract text from the PDF link\n",
        "pdf_text = extract_pdf_text(pdf_path)\n",
        "\n",
        "# Create a DataFrame with the entire content\n",
        "data = [\n",
        "    {\n",
        "        'title': \"Form 1-Z\",\n",
        "        'heading': \"Full Content\",\n",
        "        'content': pdf_text,\n",
        "        'tokens': len(pdf_text.split())\n",
        "    }\n",
        "]\n",
        "\n",
        "# Create a DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "df.to_csv(\"output.csv\", index=False)\n",
        "\n",
        "print(\"CSV file generated with the full content of the PDF.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6PnhSGz4wV7J",
        "outputId": "5790a989-fb46-40d6-e23b-20402e84b368"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted Text from PDF:\n",
            "CSV file generated with the full content of the PDF.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Converting Text content of pdf to Embeddings\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "# Load the Universal Sentence Encoder model\n",
        "module_url = \"https://tfhub.dev/google/universal-sentence-encoder/4\"\n",
        "embed = hub.load(module_url)\n",
        "\n",
        "# Load your CSV data\n",
        "df = pd.read_csv(\"/content/output.csv\")\n",
        "\n",
        "# Convert the 'content' column to embeddings\n",
        "embeddings = embed(df['content'])\n",
        "\n",
        "# Create a new DataFrame with 'text' and 'embedding' columns\n",
        "result_df = pd.DataFrame({'text': df['content'], 'embedding': embeddings.numpy().tolist()})\n",
        "\n",
        "# Save the new DataFrame to a CSV file\n",
        "result_df.to_csv(\"output_with_embeddings.csv\", index=False)\n",
        "\n",
        "print(\"CSV file generated with 'text' and 'embedding' columns.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jNGWy-Usxywu",
        "outputId": "af1d5591-413a-4e90-bd3d-577ec6a7c62b"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CSV file generated with 'text' and 'embedding' columns.\n"
          ]
        }
      ]
    }
  ]
}